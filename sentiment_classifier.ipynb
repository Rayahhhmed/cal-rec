{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting words to json file\n",
    "def get_delimited_text(text_file): \n",
    "        word_list = []\n",
    "        with open(text_file, 'r') as fp:\n",
    "            for line in fp: \n",
    "                if line[0] not in [' ', ';']:\n",
    "                    line = line.replace('\\n', \"\");\n",
    "                    if len(line) > 0:\n",
    "                        word_list.append(line)\n",
    "        return word_list\n",
    "\n",
    "\n",
    "def convert_to_json(): \n",
    "    negative_words = []\n",
    "    positive_words = []\n",
    "    negative_words = get_delimited_text('negative-words.txt')\n",
    "    positive_words = get_delimited_text('positive-words.txt')\n",
    "    corpus = []\n",
    "    for i in negative_words: \n",
    "        corpus.append((i, 'neg'))\n",
    "    for i in positive_words: \n",
    "        corpus.append((i, 'pos'))\n",
    "    return corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Creating my own CV\n",
    "random.seed(2)\n",
    "N = 2\n",
    "def cross_validation (data, folds_n=5) :\n",
    "    random.shuffle(data)\n",
    "    X = data\n",
    "    fold_size = len(data) // folds_n\n",
    "    start = N * fold_size \n",
    "    end = start + fold_size\n",
    "    # Make sure the Fold dont overlap, lazy to check defensively\n",
    "    return {\"X_train\" : X[:start] + X[end:], \"X_test\": X[start:end]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Naive Based Classifier\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "X_train, X_test = cross_validation(convert_to_json())[\"X_train\"], cross_validation(convert_to_json())[\"X_test\"]\n",
    "classifier = NaiveBayesClassifier(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package names to /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/raiyan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(['vader_lexicon', 'movie_reviews',  'names', 'averaged_perceptron_tagger', 'twitter_samples'])\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sentim_analyzer, open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
